---

siteName: narvi
intDomain: tcsc-local
siteDomain: cc.tut.fi
nodeBase: na
nodeGpuBase: nag

# === ARC confs === #

voName: "fgi.csc.fi"
instSiteURL: "http://www.tut.fi/tcsc"
gocDBName: "FI_TUT"
#infoSysAdminDomain: "tut.fi"
infoSysAdminDomain: "FI_TUT"
resource_location: "Tampere, Finland"
resource_latitude: "60.18"
resource_longitude: "24.83"

ext_gateway: "130.230.56.1"
ext_net_addr: "130.230.56.0"
ext_net_mask: "255.255.255.0"

int_net_addr: "10.1.10.0"
int_net_mask: "255.255.0.0"
int_gateway: "10.1.10.4"
internal_net: "10.1.0.0/16"
mgmt_net: "10.3.0.0/16"

ib_net_mask: "255.255.0.0"

#dns_resolv_search: "{{ intDomain }}.{{ siteDomain }}"
dns_resolv_search: "{{ intDomain }}"
nameserver1: "10.1.10.2"
nameserver2: "10.1.1.2"
nameserver3: "130.230.24.10"
nameserver4: "193.166.4.25"

#dhcp_common_domain: "{{ intDomain }}.{{ siteDomain }}"
dhcp_common_domain: "{{ intDomain }}"

enable_ext_nic: "yes"
enable_int_nic: "yes"

infiniband_available: False

# NFS mounts

nfs_mount_addr: "10.2.10.5"
scratch_nfs_addr: "10.2.1.98"
nfs_oldhome: "10.2.1.5"
fys_nfs: "10.2.1.6"
sgn_nfs: "10.2.1.7"
fys_data: "10.2.1.9"
sgn_data: "10.2.1.20"
bmt_data: "10.2.3.12"

nfs_mounts:
 - { name: "/home", src: "{{ nfs_mount_addr }}:/home" }
 - { name: "/scratch", src: "{{ scratch_nfs_addr }}:/scratch" }
 - { name: "/sgn", src: "{{ sgn_nfs }}:/sgn", opts: 'rsize=1048576,wsize=1048576,intr' }
 - { name: "/fys", src: "{{ fys_nfs }}:/fys", opts: 'rsize=1048576,wsize=1048576,intr' }
 - { name: "/sgn-data", src: "{{ sgn_data }}:/sgn-data", opts: 'rsize=1048576,wsize=1048576,intr,noacl' }
 - { name: "/fys-data", src: "{{ fys_data }}:/data", opts: 'rsize=1048576,wsize=1048576,intr,noacl' }
 - { name: "/bmt-data", src: "{{ bmt_data }}:/bmt-data", opts: 'rsize=1048576,wsize=1048576,intr,noacl' }
# - { fstype: "nfs4", name: "/home", src: "{{ nfs_mount_addr }}:/home", state: "mounted", opts: "defaults" }

#nfs_mount: |
#  {{ scratch_nfs_addr }}:/scratch /scratch nfs4 defaults 0 0
#  {{ nfs_mount_addr }}:/home    /home   nfs4    defaults 0 0
#  {{ nfs_fys }}:/fys            /fys    nfs4    rsize=32768,wsize=32768,intr    0 0
#  {{ nfs_sgn }}:/sgn            /sgn    nfs4    rsize=32768,wsize=32768,intr    0 0
#  {{ fys_data }}:/data      /fys-data   nfs4    rsize=1048576,wsize=1048576,intr,noacl  0 0 
#  {{ sgn_data }}:/sgn-data  /sgn-data   nfs4    rsize=1048576,wsize=1048576,intr,noacl  0 0 
#  {{ bmt_data }}:/bmt-data  /bmt-data	nfs4	rsize=1048576,wsize=1048576,intr,noacl  0 0

#  10.2.1.5:/home  /home                   nfs4    rsize=32768,wsize=32768,intr    0 0
#  10.2.1.6:/fys   /fys                    nfs4    rsize=32768,wsize=32768,intr    0 0
#  10.2.1.7:/sgn           /sgn                    nfs4    rsize=1048576,wsize=1048576,intr        0 0
#  merope-nfs4-ib:/data    /fys-data               nfs4    rsize=1048576,wsize=1048576,intr,noacl  0 0
#  merope-nfs5:/sgn-data     /sgn-data               nfs4    rsize=1048576,wsize=1048576,intr 0 0

repo_location: "http://www.nic.funet.fi/pub/mirrors/centos.org/7/os/x86_64/"
repos:
  - { name: "EPEL", url: "http://www.nic.funet.fi/pub/mirrors/fedora.redhat.com/pub/epel/7/x86_64", pkgname: "epel-release" }
  - { name: "Mellanox-IB", url: "http://{{ kickstart_server_ip }}/Mellanox-repo" }
    when "{{infiniband_available}"
    
#  - { name: "EL Repo", url: "http://elrepo.org/linux/elrepo/el7/x86_64", pkgname: "elrepo-release" }

yum_repos:
  - { name: "Mellanox-IB", description: "Mellanox IB-stack repo", url: "http://{{ kickstart_server_ip }}/Mellanox-repo/{{ ansible_distribution_major_version }}", gpgkey: "http://{{ kickstart_server_ip }}/RPM-GPG-KEY-Mellanox", gpgcheck: no, enabled: yes }

# - name: add local defined yum repositories
#yum_repository:
#name: "{{ item.name }}"
#description: "{{ item.name }}"
#baseurl: "{{ item.url }}"
#gpgkey: "{{ item.gpgkey | default(omit) }}"
#gpgcheck: "{{ item.gpgcheck | default(omit) }}"
#enabled: "{{ item.enabled | default(omit) }}"
#exclude: "{{ item.exclude | default(omit) }}"
#state: "{{ item.state | default(omit) }}"
#with_items: "{{ yum_repos | default({}) }}"

hyper: "{{siteName}}-admin.{{siteDomain}}"
bastion_host: "{{siteName}}-admin.{{siteDomain}}"
http_proxy: "http://{{ int_gateway }}:3128"
env_virt_install: { http_proxy: "{{ http_proxy }}" }
#libvirt_pool_name: "vms"
libvirt_pool_name: "KVM"
#libvirt_pool_type: "dir"
libvirt_pool_type: "logical"
#libvirt_pool_path: "/var/{{ libvirt_pool_name }}"
libvirt_pool_path: "/dev/vg_narvia"
lvm_group: "vg_narvia"
libvirt_pool_dev: "/dev/sdb"
# The @ for UDP. @@ for TCP
central_log_host: "@{{siteName}}-admin"

# DEVSYS  - to change later
vm_mem: "16000"
vm_cpus: "4"

# No firewalld anywhere
disable_packages:
 - "firewalld"

# Ferm - firewall
# Ordering is important. This rule is a default and is added early in the iptables rules.
ferm_rules:
 05_ssh:
 - chain: INPUT
   domains: [ip, ip6]
   rules:
     - {rule: "policy DROP;",  comment: "global policy"}
     - {rule: "mod state state INVALID DROP;", comment: "connection tracking: drop"}
     - {rule: "mod state state (ESTABLISHED RELATED) ACCEPT;", comment: "connection tracking"}
     - {rule: "interface lo ACCEPT;", comment: "allow local packet"}
     - {rule: "proto icmp ACCEPT;", comment: "respond to ping"}
     - {rule: "proto tcp dport (22) saddr ( {{ trusted_public_networks }} {{ trusted_public_ipv6_networks }} )mod comment comment 'SSH' ACCEPT;", comment: "SSH"}
     - {rule: "interface {{ internal_interface }} mod comment comment 'Allow all internal' ACCEPT;", comment: "internal"}
     - {rule: "interface ( ib0 ib1 ) mod comment comment 'Allow all internal ib traffic' ACCEPT;", comment: "internal"}
 - chain: OUTPUT
   domains:
     - ip
     - ip6
   rules:
     - rule: "policy ACCEPT;"
       comment: global policy
 - chain: FORWARD
   domains: [ip, ip6]
   rules:
     - rule: "policy DROP;"
       comment: global policy
     - rule: "mod state state INVALID DROP;"
       comment: "connection tracking: drop"
     - rule: "mod state state (ESTABLISHED RELATED) ACCEPT;"
       comment: "connection tracking"
 
# CVMFS - if fgci_install is set to False it does not install the CVMFS configs from FGCI repo
fgci_install: True
# set cms_site and we create /etc/cvmfs/config.d/cms.cern.ch.local
# cms_site: "T2_FI_HIP"

# E-mail aliases
aliases:
 - { user: "root", alias: "{{ adminMailAddr }}" }

# SSHD
# Default in CentOS7 is that GSSAPICleanupCredentials is "no". https://bugzilla.redhat.com/show_bug.cgi?id=1055016
sshd:
 GSSAPICleanupCredentials: "no"
 PermitRootLogin: "without-password"
 PasswordAuthentication: "no"

#NTP
ntp_config_server: [ ntp.cc.tut.fi, 2.fi.pool.ntp.org, 3.fi.pool.ntp.org, ntp2.funet.fi, ntp4.funet.fi]
#Chrony
chrony_ntp_servers: "{{ ntp_config_server }}"
chrony_pkg_state: "installed"
chrony_pkg_update_cache: "no"
chrony_remove_ntp: True
# We make chronyd not act as a server by default by not allowing any subnets
chrony_allow_deny:
  - ""


#Users
adminremove_passwords: True

#Flowdock
# set flowdock_token: "TOKEN" to prevent sending flowdock messages to #fgci
flowdock_token: 22a225c10814863ea809b80940d25b0e

#Collectd
collectd_tsdb_writer: True
collectd_tsdb_host: cassini.fgci.csc.fi
collectd_tsdb_tags: "site={{ siteName }}"
collectd_network_listener: False
collectd_network_server: False
collectd_plugin_packages:
 - collectd-write_tsdb

kickstart_server_ip: 10.1.10.2
pull_install_ip: "{{ kickstart_server_ip }}"

#NIS
nis_domain: "narvi"
nis_server_address: "10.1.10.2"
nis_secure_net: "255.255.0.0 10.1.0.0"

idmapd_domain: "merope"
idmapd_enabled: True

#ansible-pull is disabled here by default. Enabled per group
ansible_pull_kickstart: false
# Random max delay in seconds
ansible_pull_sleep: 600
# Defaults to "0 */2 * * *" - every second hour
ansible_pull_cron_minute_interval: "0"
ansible_pull_cron_hour_interval: "*/2"
# This is the branch of fgci-ansible repo in github.
ansible_pull_branch: "master"
# Set the ansible_pull_cron_state to absent to disable the ansible-pull cronjob
ansible_pull_cron_state: present
# Store ansible-pull logs longer than just the last run
ansible_pull_log: True
ansible_pull_logrotate: True
ansible_pull_logrotate_interval: daily
ansible_pull_logrotate_rotate: 16

# Only install these on the admin node
dell_install_helper_scripts: False

# Define your SMTP server here
postfix_relayhost: "narvi-eth.local"
postfix_mydomain: "{{ siteDomain }}"

m630_nodes: "me[156-158,280]"
sl230_nodes: "me[55-56]"

fgci_slurmrepo_version: "fgcislurm1605"
slurm_topology_plugin: "topology/tree"
slurm_topologylist:
  - "##########################"
  - "# Narvi topology"
  - "##########################"
  - "SwitchName=s0 Switches=s[1-4]"
  - "SwitchName=s1 Nodes=nag"
  - "SwitchName=s2 Nodes=na[01-32]"
  - "SwitchName=s3 Nodes={{ m630_nodes }}"
  - "SwitchName=s4 Nodes={{ sl230_nodes }}"

#slurm_topologylist: |
#  ##########################
#  # Narvi topology
#  ##########################
#  SwitchName=s0 Switches=s[1-2]
#  SwitchName=s1 Nodes=na[01-08],gpu
#  SwitchName=s2 Nodes=na[09-32]

slurm_nodelist:
 - "NodeName=na[01-32] RealMemory=126000 Weight=10 Sockets=2 CoresPerSocket=12 ThreadsPerCore=1 State=UNKNOWN"
 - "NodeName=nag Gres=gpu:teslak80:3 Weight=100 RealMemory=126000 Sockets=2 CoresPerSocket=12 ThreadsPerCore=1 State=UNKNOWN"
 - "NodeName={{ m630_nodes }} RealMemory=64000 Weight=50 Sockets=2 CoresPerSocket=12 ThreadsPerCore=1 State=UNKNOWN"
 - "NodeName={{ sl230_nodes }} RealMemory=128000 Weight=20 Sockets=2 CoresPerSocket=8 ThreadsPerCore=1 State=UNKNOWN"

#slurm_partitionlist:
# - "PartitionName=batch Nodes={{ slurm_compute_nodes }} Default=NO MaxTime=INFINITE State=UP DefaultTime=2:00:00"
# - "PartitionName=grid Nodes=gpu[1-1234] Default=NO MaxTime={{ slurm_max_job_time }} State=UP DefaultTime=2:00:00"
#
slurm_partitionlist:
 - "PartitionName=normal Nodes={{ slurm_compute_nodes }},{{ m630_nodes }},nag Default=YES MaxTime=7-00:00:00 State=UP DenyAccounts=Students DefaultTime=2:00:00"
 - "PartitionName=test Nodes=na[01-32],nag,{{ m630_nodes }},{{ sl230_nodes }} Default=YES MaxNodes=1 MaxCPUsPerNode=8 MaxTime=0-04:00:00 State=UP DefaultTime=0:15:00"
 - "PartitionName=grid Nodes=na[01-32] Default=NO MaxTime=7-00:00:00 State=UP DenyAccounts=local DefaultTime=2:00:00"

slurm_plugstack: True
slurm_x11_spank: True

# Vahan tiukempi time-limit forcing
slurm_OverTimeLimit: "15"

# Vaatii ilmeisesti drainin konffatessa:
#slurm_JobAcctGatherType: "jobacct_gather/linux"
#slurm_JobAcctGatherFrequency: "60"

slurm_ExtraParamsList:
        - "# Extra Slurm parameter *uncommented* lines go here"
        - "# Check ansible-role-slurm defaults file for examples"

hourly_update_level: "default"
# Slurm compute nodes: Change this to match the number of nodes in your cluster  
slurm_compute_nodes: "{{ nodeBase }}[01-32]"
slurm_compute_threadspercore: 1
slurm_greslist:
  - "NodeName={{ nodeGpuBase }} Name=gpu Type=teslak80 File=/dev/nvidia[0-2]"


#hosts_file_pxe_group_to_populate: "{{ groups.production }}"
hosts_file_pxe_group_to_populate: "{{ groups.pxe_bootable_nodes }}"
#hosts_file_pxe_group_to_populate: "{{ groups.with_mdadm_raid }}"
#hosts_file_pxe_group_to_populate: ""
hosts_file_admin_group_to_populate: ""
hosts_file_grid_group_to_populate: "{{ groups.grid }}"
hosts_file_install_group_to_populate: "{{ groups.install }}"
hosts_file_login_group_to_populate: "{{ groups.login }}"
hosts_file_extra_group_to_populate: ""


